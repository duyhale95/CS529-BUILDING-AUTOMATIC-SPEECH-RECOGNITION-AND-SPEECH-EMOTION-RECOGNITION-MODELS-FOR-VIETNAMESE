{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chuẩn bị dữ liệu ASR với bộ 28k Vietnamese Voice\n",
        "\n",
        "Notebook này thực hiện các bước:\n",
        "\n",
        "1. Tải bộ dữ liệu `natmin322/28k_vietnamese_voice_augmented_of_VigBigData` từ Hugging Face.\n",
        "2. Chuẩn hóa cấu trúc dữ liệu cho bài toán ASR:\n",
        "   - Chuẩn audio về sampling rate 16 kHz, mono (dùng cột `audio` của Hugging Face Datasets).\n",
        "   - Chuẩn hóa trường transcript (cột `sentence`).\n",
        "3. Gộp các split train (`train_1` → `train_5`) thành một tập train lớn.\n",
        "4. Chia lại thành `train` và `validation` (90% / 10%).\n",
        "5. Giữ nguyên split `test` gốc của dataset.\n",
        "6. Lưu lại các split dưới dạng Hugging Face Dataset bằng `save_to_disk` để dùng cho huấn luyện ASR sau này.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'datasets'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Cài đặt thư viện cần thiết (chạy một lần trong môi trường mới)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Nếu đã có rồi thì có thể bỏ qua cell này\u001b[39;00m\n\u001b[0;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstall -q datasets soundfile librosa tqdm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset, concatenate_datasets, Audio\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\haled\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\haled\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\haled\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tokenizers 0.14.1 requires huggingface_hub<0.18,>=0.16.4, but you have huggingface-hub 1.1.5 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "# Cài đặt thư viện cần thiết (chạy một lần trong môi trường mới)\n",
        "# Nếu đã có rồi thì có thể bỏ qua cell này\n",
        "%pip install -q datasets soundfile librosa tqdm\n",
        "\n",
        "from datasets import load_dataset, concatenate_datasets, Audio\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Thư mục lưu dataset đã xử lý\n",
        "OUTPUT_DIR = Path(\"./data/asr_28k\")\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Thư mục lưu dữ liệu:\", OUTPUT_DIR.resolve())\n",
        "a\n",
        "# 1. Tải dataset 28k từ Hugging Face\n",
        "print(\"\\n[1/4] Đang tải dataset 28k Vietnamese Voice từ Hugging Face...\")\n",
        "vig_dataset = load_dataset(\"natmin322/28k_vietnamese_voice_augmented_of_VigBigData\")\n",
        "print(\"Các splits có sẵn:\", list(vig_dataset.keys()))\n",
        "\n",
        "# 2. Gộp các split train_1 → train_5\n",
        "print(\"\\n[2/4] Đang gộp các split train_1 → train_5...\")\n",
        "train_splits = [f\"train_{i}\" for i in range(1, 6)]\n",
        "train_datasets = [vig_dataset[split] for split in train_splits]\n",
        "\n",
        "vig_train_all = concatenate_datasets(train_datasets)\n",
        "vig_test = vig_dataset[\"test\"]\n",
        "\n",
        "print(f\"Số mẫu train (gộp): {len(vig_train_all)}\")\n",
        "print(f\"Số mẫu test: {len(vig_test)}\")\n",
        "\n",
        "# 3. Chuẩn hóa cấu trúc cho ASR: audio (16 kHz), transcript (lowercase, strip)\n",
        "print(\"\\n[3/4] Đang chuẩn hóa cấu trúc dữ liệu cho ASR...\")\n",
        "\n",
        "def prepare_asr_example(example):\n",
        "    \"\"\"Chuẩn hóa 1 sample cho bài toán ASR.\"\"\"\n",
        "    # Cột `audio` của Hugging Face đã chứa array và sampling_rate\n",
        "    audio = example[\"audio\"]\n",
        "    transcript = example[\"sentence\"]\n",
        "\n",
        "    return {\n",
        "        \"audio\": audio,\n",
        "        \"transcript\": str(transcript).lower().strip(),\n",
        "        \"duration\": float(audio[\"duration\"]),\n",
        "    }\n",
        "\n",
        "# Áp dụng cho train và test\n",
        "vig_train_processed = vig_train_all.map(\n",
        "    prepare_asr_example,\n",
        "    remove_columns=[col for col in vig_train_all.column_names if col not in [\"audio\", \"sentence\"]],\n",
        "    num_proc=4,\n",
        ")\n",
        "\n",
        "vig_test_processed = vig_test.map(\n",
        "    prepare_asr_example,\n",
        "    remove_columns=[col for col in vig_test.column_names if col not in [\"audio\", \"sentence\"]],\n",
        "    num_proc=4,\n",
        ")\n",
        "\n",
        "# Đảm bảo cột audio được cast về Audio với sampling_rate = 16 kHz\n",
        "vig_train_processed = vig_train_processed.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "vig_test_processed = vig_test_processed.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "\n",
        "print(\"Hoàn tất chuẩn hóa. Ví dụ 1 sample train:\")\n",
        "print({k: vig_train_processed[0][k] for k in [\"transcript\", \"duration\"]})\n",
        "\n",
        "# 4. Chia train thành train/validation (90% / 10%)\n",
        "print(\"\\n[4/4] Đang chia train thành train/validation (90% / 10%)...\")\n",
        "\n",
        "split_dict = vig_train_processed.train_test_split(test_size=0.1, seed=42)\n",
        "train_dataset = split_dict[\"train\"]\n",
        "val_dataset = split_dict[\"test\"]\n",
        "\n",
        "total_train = len(train_dataset)\n",
        "total_val = len(val_dataset)\n",
        "total_test = len(vig_test_processed)\n",
        "\n",
        "print(f\"Số mẫu train: {total_train}\")\n",
        "print(f\"Số mẫu val:   {total_val}\")\n",
        "print(f\"Số mẫu test:  {total_test}\")\n",
        "\n",
        "# 5. Lưu dataset đã xử lý xuống đĩa bằng save_to_disk\n",
        "print(\"\\nĐang lưu các split đã xử lý xuống đĩa...\")\n",
        "\n",
        "train_path = OUTPUT_DIR / \"train\"\n",
        "val_path = OUTPUT_DIR / \"val\"\n",
        "test_path = OUTPUT_DIR / \"test\"\n",
        "\n",
        "train_dataset.save_to_disk(str(train_path))\n",
        "val_dataset.save_to_disk(str(val_path))\n",
        "vig_test_processed.save_to_disk(str(test_path))\n",
        "\n",
        "print(\"Đã lưu:\")\n",
        "print(\"  - Train:\", train_path.resolve())\n",
        "print(\"  - Val:  \", val_path.resolve())\n",
        "print(\"  - Test: \", test_path.resolve())\n",
        "\n",
        "# 6. Lưu thống kê cơ bản vào file JSON\n",
        "print(\"\\nĐang ghi thống kê dữ liệu vào stats.json...\")\n",
        "\n",
        "train_transcripts = [ex[\"transcript\"] for ex in train_dataset]\n",
        "train_lengths = [len(t) for t in train_transcripts]\n",
        "train_durations = [ex[\"duration\"] for ex in train_dataset]\n",
        "\n",
        "stats = {\n",
        "    \"train_samples\": total_train,\n",
        "    \"val_samples\": total_val,\n",
        "    \"test_samples\": total_test,\n",
        "    \"avg_transcript_length\": float(sum(train_lengths) / len(train_lengths)),\n",
        "    \"min_transcript_length\": int(min(train_lengths)),\n",
        "    \"max_transcript_length\": int(max(train_lengths)),\n",
        "    \"avg_duration\": float(sum(train_durations) / len(train_durations)),\n",
        "    \"min_duration\": float(min(train_durations)),\n",
        "    \"max_duration\": float(max(train_durations)),\n",
        "}\n",
        "\n",
        "with open(OUTPUT_DIR / \"stats.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(stats, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"Hoàn thành chuẩn bị dữ liệu ASR cho bộ 28k Vietnamese Voice.\")\n",
        "print(\"Thống kê chính:\")\n",
        "for k, v in stats.items():\n",
        "    print(f\"  {k}: {v}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
